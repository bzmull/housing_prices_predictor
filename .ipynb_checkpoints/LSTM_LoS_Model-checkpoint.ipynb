{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length of Stay Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import csv\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from random import random\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras import optimizers\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from common_utility.IOUtility import spark_read_parquet\n",
    "from pyspark.sql import SparkSession\n",
    "from etl.Transformation import *\n",
    "\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "init_notebook_mode()\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"read parquet\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('/data/AppsFiles/batch/parquetDatabase/los/nullImputationDFV14.parquet')\n",
    ".select(\"Column Names needed to be removed due to issues regarding intellectual property\").toPandas()\n",
    "# df = spark_read_parquet(spark, \"LSTMNullImputation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ts_list = [\"Column Names needed to be due to for issues regarding intellectual property\"]\n",
    "\n",
    "num_list = ['Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_feature(df, num_ts_list, num_list, general_feature=[\"VISIT_ID\", \"label\"]):\n",
    "    '''\n",
    "    Generate feature for lstm model\n",
    "    :param df: pandas df\n",
    "    :param numeric_ts: numerical time series feature list\n",
    "    :param cat_ts: categorical time series feature list\n",
    "    :param numeric_non_ts: numerical non-time series feature list\n",
    "    :param cat_non_ts: categorical non-time series feature list\n",
    "    :param general_feature: Non-training feature, label\n",
    "    :return: pandas df\n",
    "    '''\n",
    "    feature_df = df[general_feature]\n",
    "    for feature in num_ts_list:\n",
    "        temp = create_lstm_numerical_array_feature(df, feature)\n",
    "        feature_df = pd.concat([temp, feature_df], axis=1)\n",
    "\n",
    "    for feature in num_list:\n",
    "        temp = create_lstm_numerical_non_array_feature(df, feature)\n",
    "        feature_df = pd.concat([temp, feature_df], axis=1)\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "def create_lstm_numerical_array_feature(df, feature):\n",
    "\n",
    "    '''\n",
    "    Create Numerical Time Series columns for LSTM model\n",
    "    :param df: raw pandas df LSTM Model\n",
    "    :param feature: feature list\n",
    "    :return: pandas df\n",
    "    '''\n",
    "\n",
    "    series = df[feature].apply(lambda x: np.array(x.toArray())).values.reshape(-1, 1)\n",
    "    x_train = np.apply_along_axis(lambda x: x[0], 1, series)\n",
    "    cols = [feature + '_t' + str(i + 1) for i in list(range(7))]\n",
    "    return pd.DataFrame(x_train, columns=cols)\n",
    "\n",
    "def lstm_feature_generator(x, y):\n",
    "\n",
    "    '''\n",
    "    generate lstm feature df\n",
    "    :param x: training feature\n",
    "    :param y: training label\n",
    "    :return: np.array feature and label for training\n",
    "    '''\n",
    "\n",
    "    n_features = x.shape[1]/7\n",
    "    input_x = x.values\n",
    "    input_x = input_x.reshape(x.shape[0], 7, int(n_features))\n",
    "#     input_y = to_categorical(y, num_classes=len(np.unique(y.values)))\n",
    "    input_y = y.values\n",
    "    return input_x, input_y\n",
    "\n",
    "def create_grid_search_dict(model):\n",
    "    parser = ConfigParser(allow_no_value=True)\n",
    "    parser.read('{}resources/application.conf'.format(module_path))\n",
    "    var = ast.literal_eval(parser.get(model, \"grid_search_var_list\"))\n",
    "    grid_dict = {}\n",
    "    for hyper_param in var:\n",
    "        grid_list = ast.literal_eval(parser.get(model, hyper_param))\n",
    "        grid_dict.update({hyper_param:grid_list})\n",
    "    return grid_dict\n",
    "\n",
    "def create_early_stopping():\n",
    "    '''\n",
    "    create early stopping parameter\n",
    "    :return: early stopping list\n",
    "    '''\n",
    "    early_stopping = EarlyStopping(monitor='val_acc', patience=100, verbose=0, mode='auto')\n",
    "    return early_stopping\n",
    "\n",
    "def undersampling(y_df, x_df):\n",
    "    num_neg_labels = len(y_df[y_df['label']==0])\n",
    "    pos_label_indices = y_df[y_df['label']==1].index\n",
    "    random_indices = np.random.choice(pos_label_indices, num_neg_labels, replace=False)\n",
    "    neg_label_indices = y_df[y_df['label']==0].index\n",
    "    under_sample_indices = np.concatenate([neg_label_indices,random_indices])\n",
    "    under_sample_y_df = y_df.loc[under_sample_indices]\n",
    "    under_sample_x_df = x_df.loc[under_sample_indices]\n",
    "    return under_sample_y_df, under_sample_x_df\n",
    "\n",
    "#create ROC\n",
    "def create_roc_trace(df, label_col, pred_col):\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(df[label_col], df[pred_col])\n",
    "\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "    trace = go.Scatter(x=false_positive_rate, y=true_positive_rate,\n",
    "                        mode='lines',\n",
    "                        line=dict(width=2),\n",
    "                        name='ROC curve (area = %0.2f)' % roc_auc\n",
    "                        )\n",
    "    return trace\n",
    "\n",
    "def create_overlay_roc_curve(trace_list):\n",
    "    trace2 = go.Scatter(x=[0, 1], y=[0, 1],\n",
    "                        mode='lines',\n",
    "                        line=dict(color='navy', width=2, dash='dash'),\n",
    "                        showlegend=False)\n",
    "    data = [trace_list] + [trace2]\n",
    "    layout = go.Layout(title='<b>Receiver Operating Characteristic Curve',\n",
    "                       height=500,\n",
    "                       width=700,\n",
    "                       xaxis=dict(title='False Positive Rate',\n",
    "                                  range=[0, 1],\n",
    "                                  tick0=0,\n",
    "                                  dtick=0.1),\n",
    "                       yaxis=dict(title='True Positive Rate',\n",
    "                                  range=[0, 1],\n",
    "                                  tick0=0,\n",
    "                                  dtick=0.1))\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return iplot(fig, filename='overlaid histogram')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create feature Dataframe (explode timeseries)\n",
    "feature_df = create_lstm_feature(df, num_ts_list, num_list, general_feature=[\"label\"])\n",
    "feature_df = feature_df.drop(['Age_t8', 'Age_t9', 'Age_t10', 'Age_t11'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#separate input and output dataframe\n",
    "inputs_x = feature_df.loc[:, feature_df.columns != 'label']\n",
    "outputs_y = feature_df.loc[:, feature_df.columns == 'label']\n",
    "#perform train / validation / test split\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(inputs_x, outputs_y, test_size=0.2)\n",
    "x_train2, x_val1, y_train2, y_val1 = train_test_split(x_train1, y_train1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform undersampling\n",
    "y_train3, x_train3 = undersampling(y_train2, x_train2)\n",
    "y_val2, x_val2 = undersampling(y_val1, x_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 3D feature array from df as required for LSTM input\n",
    "x_train4, y_train4 = lstm_feature_generator(x_train3, y_train3)\n",
    "x_val3, y_val3 = lstm_feature_generator(x_val2, y_val2)\n",
    "x_test2, y_test2 = lstm_feature_generator(x_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just some renaming for simplicity\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = x_train4, y_train4, x_val3, y_val3, x_test2, y_test2\n",
    "x_train.shape, x_val.shape, x_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The Bi-LSTM Model function\n",
    "def create_BiLSTM(x_train, y_train, x_val, y_val, neurons, merge_mode, dropout, lr, batch_size, epochs):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(neurons), merge_mode=merge_mode, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "#     model.add(LSTM(neurons, input_shape=(x_train.shape[1],x_train.shape[2]), kernel_constraint=max_norm(2)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "#     optimizer = optimizers.Adam(lr=lr)\n",
    "#     optimizer = optimizers.RMSprop(lr=lr)\n",
    "    optimizer = optimizers.Nadam(lr=lr)\n",
    "    model.compile(loss='binary_crossentropy', optimizer = optimizer, metrics=['acc'])\n",
    "    early_stopping = create_early_stopping()\n",
    "    \n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, verbose=1, batch_size=batch_size, callbacks=[early_stopping],)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training the model\n",
    "model = create_BiLSTM(x_train, y_train, x_val, y_val, neurons=200, merge_mode='ave', dropout=0.01, lr=0.0005, batch_size=500, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set: Specificity and Sensitivity \n",
    "y_pred_test = model.predict_classes(x_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "test_spec = tn/(tn+fp)\n",
    "test_sens = tp/(tp+fn)\n",
    "test_spec, test_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train set: Specificity and Sensitivity\n",
    "y_pred_train = model.predict_classes(x_train)\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, y_pred_train).ravel()\n",
    "train_spec = tn/(tn+fp)\n",
    "train_sens = tp/(tp+fn)\n",
    "train_spec, train_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set: f1 Score\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train set: f1 Score\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "f1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set: ROC curve\n",
    "y_test_score = model.predict(x_test)\n",
    "d = {'test_labels':y_test.reshape(y_test.shape[0]).tolist(), 'test_probabilities':y_test_score.reshape(y_test_score.shape[0]).tolist()}\n",
    "test_prob_df = pd.DataFrame(data=d)\n",
    "roc_trace = create_roc_trace(test_prob_df, 'test_labels', 'test_probabilities')\n",
    "create_overlay_roc_curve(roc_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train set: ROC curve\n",
    "y_train_score = model.predict(x_train)\n",
    "d = {'train_labels':y_train.reshape(y_train.shape[0]).tolist(), 'train_probabilities':y_train_score.reshape(y_train_score.shape[0]).tolist()}\n",
    "train_prob_df = pd.DataFrame(data=d)\n",
    "roc_trace = create_roc_trace(train_prob_df, 'train_labels', 'train_probabilities')\n",
    "create_overlay_roc_curve(roc_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
